import numpy as np 

import pandas as pd

import matplotlib.pyplot as plt

#  load dataset 

dataset = pd.read_csv(r"C:\Users\jrbil\OneDrive\Desktop\Data science\Machine Learning\Salary_Data.csv")

# split the dataset to independent and dependent variable 

x = dataset.iloc[:,:-1]
y = dataset.iloc[:,1]

# as dependent variable is continuous we use regression algorithm
# split the dataset to 80-20%
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8, random_state=0)

# we called simple linear regression from sklearn freamework
from sklearn.linear_model import LinearRegression
regressor = LinearRegression() # regressor is used for continuous dependent variable (y)
regressor.fit(x_train,y_train) # .fit is used to train the model

# test the model & create a predicted table 
y_pred = regressor.predict(x_test)

comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(comparison)
 
# visualization for train the data points
plt.scatter(x_train, y_train, color = 'red') 
plt.plot(x_train, regressor.predict(x_train), color = 'blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

# visualization for test the data points 
plt.scatter(x_test, y_test, color = 'red') 
plt.plot(x_train, regressor.predict(x_train), color = 'blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

# slope is generrated foe linear regession algorithm which fit to data set
m_slope =regressor.coef_
print(m_slope)

# intercept also generated by model
c_intercept = regressor.intercept_
print(c_intercept)

# predict or forcast the future the data which which are we not trained before
y_12 = m_slope*12+c_intercept
print(y_12)

# check model performance 
# to check underfitting 
bias = regressor.score(x_train, y_train) # training_score(r2)
print(bias)

# to check overfitting  
variance = regressor.score(x_test,y_test) # testing_score (r2)
print(variance)

# training and testing of mean suqare root error(MSE)
from sklearn.metrics import mean_squared_error
train_mse = mean_squared_error(y_train, regressor.predict(x_train))
test_mse = mean_squared_error (y_test, y_pred)
# mean, median , mode , variance and standard deviation
dataset.mean()
dataset['Salary'].mean()

dataset.median()
dataset['Salary'].median()

dataset.mode()
dataset['Salary'].mode()[0]


dataset.var()
dataset['Salary'].var()

dataset.std()
dataset['Salary'].std()

#coefficient of variation
# scipy is the statistical library
from scipy.stats import variation
variation(dataset.values)
variation(dataset['Salary'])

# corelation 
dataset['Salary'].corr(dataset['YearsExperience'])

# Skewness , = 0 -> perfectly symmetric , > 0 right skew, < 0 left skew
dataset.skew()
dataset['Salary'].skew()

# Standard Error
dataset.sem()
dataset['Salary'].sem()

# Z-score ( range is -3 to 3)
import scipy.stats as stats
dataset.apply(stats.zscore)
stats.zscore(dataset['Salary'])

# degree of freedom 
a = dataset.shape[0] # indicates rows
b = dataset.shape[1] # indicate columns 
degree_of_freedom = a-b
print(degree_of_freedom)

# SSR
y_mean = np.mean(y)
SSR = np.sum(y_pred-y_mean)
print(SSR)

# SSE
y = y[0:6]
SSE = np.sum((y-y_pred)**2)
print(SSE)

# SST
mean_total = np.mean(dataset.values)
SST = np.sum((dataset.values-mean_total)**2)
print(SST)

#R2 squre
r_squre = 1-(SSR/SST)
print(r_squre)

# save the trained model to disk
import pickle
filename = 'linear_regression_model.pkl'
with open(filename, 'wb') as file:
    pickle.dump(regressor, file)
print("Model has been pickled and saved as linear_regression_model.pkl")